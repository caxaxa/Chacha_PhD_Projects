{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Game of Corruption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "* Settle the Basic Model, the relevant constants and variables; \n",
    "\n",
    "* Build the Transition functions;\n",
    "\n",
    "* Build the Reward Function;\n",
    "\n",
    "* Find the optimal policy from one player given a greedy policy from the other player; and\n",
    "\n",
    "* Use optimal policicies found and iterate until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quantecon as qe\n",
    "import scipy.sparse as sparse\n",
    "from quantecon import compute_fixed_point\n",
    "from quantecon.markov import DiscreteDP\n",
    "%load_ext itikz\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Variables\n",
    "\n",
    "* Discrete time $t$;\n",
    "\n",
    "* Two players $i$ =  $payer$ and $receiver$;\n",
    "\n",
    "* Bribes are fixed $b$;\n",
    "\n",
    "* Gains from corruption are $a$;\n",
    "\n",
    "* Cost from perfoming the corruption favour $c$ ;\n",
    "\n",
    "* Monetary Fines are $F$;\n",
    "\n",
    "* Agents wealth $w_t$;\n",
    "\n",
    "* $\\delta$ is the time discount; and\n",
    "\n",
    "* Probability of detection is $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runing a computational excercize:\n",
    "\n",
    "#Constants\n",
    "b = 3\n",
    "c = 1\n",
    "a = 5\n",
    "F = 7\n",
    "alpha = 0.25\n",
    "delta = 0.98\n",
    "y0 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to write the corruption in terms of payoffs $\\pi$ and costs $\\phi$ for players.\n",
    "\n",
    "Or else, $\\pi_{payer} = a$ and, $\\pi_{receiver} = b$. Also, $\\phi_{payer} = b$ and, $\\phi_{receiver} = c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(player):\n",
    "    if player == 'payer':\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "    \n",
    "def phi(player):\n",
    "    if player == 'payer':\n",
    "        return b\n",
    "    else:\n",
    "        return c    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Actions:\n",
    "\n",
    "This example is built using discrete time, decisions and states:\n",
    "\n",
    "* Discrete (countable) states $\\textbf{x}_{i,t}$;\n",
    "\n",
    "* Discrete actions or decisions (controls)  $\\textbf{d}_{i,t}$\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\mathbf{d_t} \\equiv\n",
    "\t\\begin{bmatrix}\n",
    "\t\td_{i,t} \\\\\n",
    "\t\td_{j,t} \n",
    "\t\\end{bmatrix}\\textrm{, and }\n",
    "\t\\mathbf{x_t}\\equiv\n",
    "\t\\begin{bmatrix}\n",
    "\t\tw_{i,t}\\\\\n",
    "\t\tw_{j,t}\\\\\n",
    "\t\ts_t\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "* $d_{i,t}$ is the decision from player $i$ to pay/receive a bribe in time $t$;\n",
    "\n",
    "* $s_t$ is the state of the world at time $t$;\n",
    "\n",
    "* $w_{i,t}$ is the wealth from player $i$ at time $t$. It goes from 0 to a maximum of $\\overline{w}$;  and\n",
    "\n",
    "* Lastly, $(s_t, w_{i,t}, w_{j,t} ) \\in x_{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximun wealth from corruption:\n",
    "\n",
    "w_bar = 30\n",
    "\n",
    "#create the wealth statespace:\n",
    "wi_space = np.arange(0,w_bar+1)\n",
    "wj_space = np.arange(0,w_bar+1)\n",
    "\n",
    "#creat the actionspace:\n",
    "\n",
    "di_space = np.array([0,1])\n",
    "dj_space = np.array([0,1])\n",
    "\n",
    "# state of the world space\n",
    "\n",
    "s_nc  = 0\n",
    "s_cor = 1\n",
    "s_det = 2\n",
    "\n",
    "s_space = np.array([s_nc,s_cor,s_det])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laws of motion\n",
    "\n",
    "$$\\mathbf{x}_{t+1} = f ( \\mathbf{x}_t , \\mathbf{d}_t , \\epsilon_{t+1} ; \\theta ) $$\n",
    "\n",
    "Where $\\theta$ is a vector of parameters.\n",
    "\n",
    "In this specific example we have:\n",
    "\n",
    "$$w_{i,t+1} = w_{i,t} + \\pi (a,b|s_{t+1}) $$\n",
    "$$ s_{t+1} = f(\\epsilon, s_t|d_t) $$\n",
    "\n",
    "Where $\\epsilon \\sim Bernoulli ~ (\\alpha)$\n",
    "\n",
    "\\* There is a living wage if walth ever gets to zero \n",
    "\n",
    "Let's impose that there are states of the world in which players can be. This, formulation helps when there are more states tied to the state of the world $s$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space transition rules:\n",
    "\n",
    "Rules from $w$' from $w$:\n",
    "\n",
    "$w$' must be feasible from $w$:\n",
    "\n",
    "Rules for $w$' from $w$:\n",
    "\n",
    "* #1 and #6 : $b_i$ = 0 or $b_j$ = 0 -> p = 1 ; \n",
    " \n",
    "* #2 and #3 : $b_i$ = 1 and $b_j$ = 1| s' = s_cor -> p = 1- $\\alpha$;\n",
    " \n",
    "* #4 and #5 : $b_i$ = 1 and $b_j$ = 1 | s' = s_det -> p = $\\alpha$;\n",
    " \n",
    "* #7: s = s_det | s' = s_det -> p = 1\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laws of motion\n",
    "\n",
    "#wealth state transition Function (as function of all other states)\n",
    "def w_prime(player,w,ui,uj,s_prime): \n",
    "    if ui == 1 and uj == 1: # Note that s_nc after ui and uj == 1  does not exist\n",
    "        if s_prime == s_det:\n",
    "            if w >= F + phi(player) - y0:\n",
    "                return w + y0 - F - phi(player) #return from detection\n",
    "            else:\n",
    "                return 0\n",
    "        elif s_prime == s_cor:\n",
    "            if w <= w_bar - phi(player) - pi(player) - y0: \n",
    "                return w + y0 + pi(player) - phi(player) # return from corruption\n",
    "            else: #if \n",
    "                return w_bar\n",
    "    else:\n",
    "        if w == w_bar:\n",
    "            return w\n",
    "        elif w <= phi(player):\n",
    "            return w + y0 + 1 # living wage\n",
    "        else:\n",
    "            return w + y0\n",
    "        \n",
    "#State of the world transition Function (as function of all other states)\n",
    "\n",
    "def s_prime(ui,uj):\n",
    "    if ui == 0 or uj == 0: #rule 1 and 6\n",
    "        return s_nc\n",
    "    else:\n",
    "        return (s_cor or s_det)\n",
    "\n",
    "# The player i takes the action from j as given \n",
    "# first iteration is a greedy strategy\n",
    "\n",
    "def dj(X,di):\n",
    "    if X[2] == s_det:\n",
    "        return 0\n",
    "    else:\n",
    "        if delta*((1-alpha)*pi('receiver') - alpha*F) > 0: # avoid workin with complex numbers\n",
    "            if reward('receiver', X[1], di, 1) < delta* expected_return('receiver', X[1], di, 1):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Objective Function\n",
    "\n",
    "$$ \\max E_0 \\sum^{\\infty}_{t=0} \\delta^t u( w_{t}) $$\n",
    "\n",
    "For some reason, it is better to have money now than later (It's possible to set $\\delta = 0$).\n",
    "\n",
    "Therfore $u(.)$ is a CCRA function, where;\n",
    "\n",
    "$$u(w) = \\frac{w ^{(1-\\eta)} - 1 } {1-\\eta} $$\n",
    "\n",
    "Where $\\eta$ is the risk aversion parameter. Where $\\eta > 0$ represents some degree of risk aversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward function:\n",
    "def reward(player,wi,ui,uj):\n",
    "#    return y - b*u #neutral risk aversion\n",
    "    if wi - phi(player)*ui*uj >= 0:\n",
    "        return (((wi - phi(player)*ui*uj)**(1-eta))-1)/(1-eta)\n",
    "    else:\n",
    "        return 0\n",
    "eta = 0.9 # if eta =1 ln(y)\n",
    "\n",
    "def expected_return(player,wi,ui,uj):\n",
    "    if wi - phi(player)*ui*uj >= 0:\n",
    "        return  ((((1-alpha)*pi(player) - alpha*F + wi )**(1-eta))-1)/(1-eta)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction:\n",
    "\n",
    "Instead of working with the entire space of controls and states (Sparce Matrices). One can reduce the number of state-acion space to only allowed movements in possible states, so the matrices decrease in dimension.\n",
    "\n",
    "For instance in this exammple, the following rules apply for the states:\n",
    "\n",
    "1 - It is not possible to pay a bribe if you are in the detected state of the world ;\n",
    "\n",
    "2 - There are budget constraints. Or else, It is not possible to pay a bribe without funds;\n",
    "\n",
    "3 - If agent succeded in bribing, state $w$ will be at least $\\pi$:\n",
    "\n",
    "4 - If agents are detected their wealth is going to be at most $\\overline{w} - \\phi - F$\n",
    "\n",
    "\n",
    "PS: In the state transition it is necessary to impose limits to the transitions to maintain the system inside of the boundaries of feasible states. Or else, the sistem transiton cannot lead to state outside the state space. Here, we are excluding state and action pairs that would never be possible to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are n = 3737 distict spaces-action pairs where players can be. Where m = 2169 are unique spaces\n",
      "Without the dimension reductions, we would be working with n = 5400 and m = 2700\n"
     ]
    }
   ],
   "source": [
    "#Build the X and U space:\n",
    "\n",
    "X = [] #set of feasible states corresponding to the action\n",
    "D = [] #set of feasible actions corresponding to the space\n",
    "XD = [] #creat the vector of the state action pair\n",
    "X_indices = []\n",
    "\n",
    "count = 0\n",
    "index = 0\n",
    "for i in wi_space:\n",
    "    for j in wj_space:\n",
    "        for k in s_space:\n",
    "            for l in di_space:\n",
    "                if k == s_det and (l == 1 or dj(np.asarray([i,j,k]),l) == 1): #rule 1\n",
    "                    pass\n",
    "                elif (i < phi('payer') and l == 1) or (j < phi('receiver') and dj(np.asarray([i,j,k]),l) == 1) : #rule2\n",
    "                    pass\n",
    "                elif k == s_cor and (i < pi('payer') + y0 or j < pi('receiver') + y0): #rule3\n",
    "                    pass\n",
    "                elif k == s_det and (i > w_bar - phi('payer') - F + y0 or j > w_bar - phi('receiver') - F + y0): #rule4\n",
    "                    pass\n",
    "                else:\n",
    "                    X.append([i,j,k])\n",
    "                    D.append(l)\n",
    "                    XD.append([i,j,k,l])\n",
    "                    #if the states are ordered, then the clauses bellow gives the X_indices\n",
    "                    if index == 0:\n",
    "                        X_indices.append(count)\n",
    "                    elif X[index] == X[index-1]:\n",
    "                        X_indices.append(count)\n",
    "                    else:\n",
    "                        count += 1\n",
    "                        X_indices.append(count)\n",
    "                    index += 1\n",
    "#                         X = np.vstack([X,(i,j,k)])\n",
    "#                         D = np.vstack([D,(l,m)])\n",
    "#                         XD= np.vstack([XD ,(i,j,k,l,m)])\n",
    "X = np.asarray(X) # converts integers to string\n",
    "D = np.asarray(D)\n",
    "XD = np.asarray(XD)\n",
    "\n",
    "#Calculating the indices\n",
    "#Indices are the maps unique actions and states\n",
    "\n",
    "X_unique, indices = np.unique(X, axis=0, return_index=True) #new X space, only with unique feasible states\n",
    "\n",
    "X_unique = X_unique[np.argsort(indices)] #return unique elements and maintain the original order\n",
    "\n",
    "X_indices = np.asarray(X_indices)\n",
    "\n",
    "D_unique, indices = np.unique(D, axis=0, return_index=True) #new X space, only with unique feasible states\n",
    "\n",
    "D_unique = D_unique[np.argsort(indices)] #return unique elements and maintain the original order\n",
    "\n",
    "D_indices = D\n",
    "\n",
    "print('There are n = {} distict spaces-action pairs where players can be. Where m = {} are unique spaces'.format(len(X) , len(X_unique)))\n",
    "print('Without the dimension reductions, we would be working with n = {} and m = {}'.format((w_bar*w_bar*len(di_space)*len(s_space)),(w_bar*w_bar*len(s_space)) ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent's problem:\n",
    "\n",
    "\n",
    "$$ \\max E_0 \\sum^{\\infty}_{t=0} \\delta^t u( w_{t}) $$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$$w_{i,t+1} = w_{i,t} + \\pi (a,b|s_{t+1}) $$\n",
    "\n",
    "$$ s_{t+1} = f(\\epsilon, s_t|d_t) $$\n",
    "\n",
    "\n",
    "It is possible to rewrite the agent's problem as a value function, such as:\n",
    "\n",
    "$$V(\\mathbf{x}) = \\max_{\\mathbf{x},\\mathbf{x'},\\mathbf{d}} \\{ u(\\mathbf{x}) + \\delta E[V(\\mathbf{x'})|\\mathbf{x},\\mathbf{d}]\\}$$\n",
    "\n",
    "This is called the Bellman Equation and it is quivallento to:\n",
    "\n",
    "$$V(\\mathbf{x}) = \\max_{\\mathbf{x},\\mathbf{x'},\\mathbf{d}} \\{ R(\\mathbf{x},\\mathbf{d}) + \\delta V(\\mathbf{x'}) \\Omega(\\mathbf{x'},\\mathbf{d},\\mathbf{x})\\}$$\n",
    "\n",
    "\n",
    "This problem can be solved by iterating the matrix $\\Omega$ through the reward function $R$.\n",
    "\n",
    "Note that the controls $\\mathbf{d}$ which solve the Bellman equation is a pair of decisions. This pair of decisions must hold at the optimum level.\n",
    "\n",
    "So, to solve for the pair or decisions $\\mathbf{d}$, first, we calculate the optimum policy for the payer $\\sigma_{payer}$ using the receiver policy $\\sigma_{receiver}$ as a greedy policy. Such that, \n",
    "\n",
    " \\begin{equation}\n",
    "\t\\sigma(\\mathbf{x}) \\in \\max_{\\mathbf{x},\\mathbf{x'},\\mathbf{u}} \\{ u(\\mathbf{x}) + \\delta \\sum_{\\mathbf{x}}  V^*(\\mathbf{x'})\\Omega(\\mathbf{x'},\\mathbf{u},\\mathbf{x})\\}\n",
    "\t\\label{v4}\n",
    "\\end{equation}\n",
    "\n",
    "The quantecom package DiscreteDP was used to calculate the strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Generating the reward vector \n",
    "\n",
    "Each player has a reward vector $\\textbf{R}$, such that:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\mathbf{R} \\equiv\n",
    "\t\\begin{bmatrix}\n",
    "\t\tR_{i} \\\\\n",
    "\t\tR_{j} \n",
    "\t\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "$R_i(x,d)$, of dimension $n$:\n",
    "\n",
    "$$R_i(\\textbf{x},\\textbf{u}) =  \\begin{bmatrix}\n",
    "r_{i,0}(x_0,d_0)\\\\\n",
    "\\vdots \\\\\n",
    "r_{i,n}(x_n,d_n) \\end{bmatrix}$$\n",
    "\n",
    "where $r_{i,j}(\\textbf{x},\\textbf{u})$ are the rewards from the state and control pair, where $i \\in [payer,receiver]$ and $j \\in [0,n]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating R for the payer:\n",
    "R = []\n",
    "for i in range(len(XD)):\n",
    "    R.append(reward('payer', X[i][0], D[i] ,dj(X[i],D[i])  ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition matrix $\\Omega(\\textbf{x},\\textbf{u},\\textbf{x}')$, of dimension $n \\times m $, can be calculated by:\n",
    "\n",
    "$\\Omega(\\textbf{x},\\textbf{u},) = \\begin{bmatrix}\n",
    " \\begin{bmatrix}\n",
    "p(x_0'|x_0,d_0) \\\\\n",
    "\\vdots\\\\\n",
    "p(x_0'|x_n,d_n) \\end{bmatrix} \\\\\n",
    "\\vdots \\\\\n",
    "\\begin{bmatrix}\n",
    "p(x_m'|x_0,d_0)\\\\\n",
    "\\vdots\\\\\n",
    "p(x_m'|x_n,d_n)  \\end{bmatrix} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Omega = np.zeros((len(X),len(X_unique)))\n",
    "#finding the X_unique from the pair XD , dj:\n",
    "for i in range(len(XD)):\n",
    "    if s_prime(XD[i][3],dj(X[i],XD[i][3])) == s_nc or XD[i][2] == s_det: # rules 1, 6 and 7\n",
    "        pos_2 = s_nc\n",
    "        pos_0 = w_prime('payer',XD[i][0],XD[i][3],dj(X[i],XD[i][3]),s_nc)\n",
    "        pos_1 = w_prime('receiver',XD[i][1],dj(X[i],XD[i][3]),XD[i][3],s_nc)\n",
    "        loc_X_unique = np.array([pos_0, pos_1, pos_2])\n",
    "\n",
    "        X_unique_prime = np.where(np.all(X_unique==loc_X_unique,axis=1))[0][0]\n",
    "        \n",
    "        Omega[i ,X_unique_prime] = 1\n",
    "        \n",
    "    else:\n",
    "        #rules 4 and 5\n",
    "        pos_2 = s_det\n",
    "        pos_0 = w_prime('payer',XD[i][0],XD[i][3],dj(X[i],XD[i][3]),s_det)\n",
    "        pos_1 = w_prime('receiver',XD[i][1],dj(X[i],XD[i][3]),XD[i][3],s_det)\n",
    "        loc_X_unique = np.array([pos_0, pos_1, pos_2])\n",
    "\n",
    "        X_unique_prime = np.where(np.all(X_unique==loc_X_unique,axis=1))[0][0]\n",
    "        \n",
    "        Omega[i ,X_unique_prime] = alpha\n",
    "        \n",
    "        #rules 2 and 3\n",
    "        pos_2 = s_cor\n",
    "        pos_0 = w_prime('payer',XD[i][0],XD[i][3],dj(X[i],XD[i][3]),s_cor)\n",
    "        pos_1 = w_prime('receiver',XD[i][1],dj(X[i],XD[0][3]),XD[i][3],s_cor)\n",
    "        loc_X_unique = np.array([pos_0, pos_1, pos_2])\n",
    "        \n",
    "        X_unique_prime = np.where(np.all(X_unique==loc_X_unique,axis=1))[0][0]\n",
    "        \n",
    "        Omega[i ,X_unique_prime] = 1 - alpha\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the Omega matrix is ther correct transition matrix if the all rows sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucess: Omega shape (n X  m) -> (3737, 2169)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = []\n",
    "fails = []\n",
    "for i in range(len(Omega)):\n",
    "    test.append(Omega[i].sum())\n",
    "    if round(Omega[i].sum(),4) != 1:\n",
    "        fails.append(i)\n",
    "if len(fails) == 0:\n",
    "    print('sucess: Omega shape (n X  m) ->', Omega.shape)\n",
    "else:\n",
    "    print('failed at the dimention(s) ->',fails)\n",
    "    print(len(fails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specically. Each positively populated value in $\\Omega$ represents the probability of transiting from that state-action pair $(x,d)\\in n$ to the future state $x' \\in m$. \n",
    "\n",
    "Or else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the pair state-action XD : [ 3 26  0  1]\n",
      "Omega item 36 computed the probability of transition of 0.25 that leads to the state :  [ 0 18  2]\n",
      "Omega item 340 computed the probability of transition of 0.75 that leads to the state :  [ 5 26  1]\n"
     ]
    }
   ],
   "source": [
    "#Use this cel to check any transition\n",
    "\n",
    "teste = 234\n",
    "\n",
    "print('From the pair state-action XD :',XD[teste])\n",
    "\n",
    "for i in range(len(Omega[1])):\n",
    "    if Omega[teste][i] !=0:\n",
    "        print('Omega item', i, 'computed the probability of transition of', Omega[teste][i], 'that leads to the state : ', X_unique[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat the QE instance for solution \n",
    "ddp =qe.markov.DiscreteDP(R, Omega, delta, X_indices, D_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ddp.solve(method='policy_iteration',max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 [10  8  0]\n"
     ]
    }
   ],
   "source": [
    "stationary = results.mc.stationary_distributions\n",
    "\n",
    "#Stationary equilibrium\n",
    "for i in range(len(stationary[0])):\n",
    "    if stationary[0,i] != 0:\n",
    "        print(round(stationary[0,i],4), X_unique[i])\n",
    "        \n",
    "#how to make it a zero summ game?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Algorithm to find Markovian Perfect Equilibrium\n",
    "\n",
    "1. Solve i's dynamic programming problem when j plays a greedy strategy as given;\n",
    "\n",
    "2. Use the optmal policy ($\\sigma_i$) from i to calculate j's optimal policy ($\\sigma_j$); and\n",
    "\n",
    "3. Compare the policies $|| \\sigma_i - \\sigma_j||$, if they are equal stop, otherwise go to step 2 alternating players. \n",
    "\n",
    "In other words, when players have no incentive to change their strategies given the other player's best responses, it is a Markov Perfect Equilibrium.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Outer Algo, we create a class that computes all the steps above.\n",
    "If we do not pass a policy $\\sigma$, the class returns the $R$ and $\\Omega$ matrices from a greedy 'other player' policy (as well as the state and action indices (X_indices and D_indices).\n",
    "However, if a policy $\\sigma$ is passed, than the best policy from 'other player' is calculated based in the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Matrices_Maker():\n",
    "\n",
    "    def __init__(self, sigma = [], X_loc = [], player = 'payer', b = 3, c = 1, a = 5, F = 7, alpha = 0.25 , delta = 0.98 , y0 = 0, w_bar = 30 , eta = 0.9):\n",
    "        \"\"\"\n",
    "        Set up an instance to run the Quantecon routine of discrete dynamic programing.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.sigma, self.X_loc, self.player, self.b,self.c,self.a,self.F,self.alpha,self.delta,self.y0,self.w_bar,self.eta = sigma, X_loc, player,b,c,a,F,alpha,delta,y0,w_bar,eta\n",
    "        \n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        if self.player == 'payer':\n",
    "            self.otherplayer = 'receiver'\n",
    "        else:\n",
    "            self.otherplayer = 'payer'\n",
    "        \n",
    "\n",
    "        # #Constants:\n",
    "        # b bribe\n",
    "        # c cost from bribe\n",
    "        # a advantage from corruption\n",
    "        # F Fine\n",
    "        # alpha Probability of conviction\n",
    "        # delta Time discount\n",
    "        # y0 Non financial income\n",
    "        # w_bar Maximum wealrh from corruption\n",
    "\n",
    "        # States and Action Spaces: \n",
    "        \n",
    "        # #create the wealth statespace:\n",
    "        self.wi_space = np.arange(0,self.w_bar+1)\n",
    "        self.wj_space = np.arange(0,self.w_bar+1)\n",
    "\n",
    "        # #creat the actionspace:\n",
    "\n",
    "        self.di_space = np.array([0,1])\n",
    "        self.dj_space = np.array([0,1])\n",
    "\n",
    "        # # state of the world space\n",
    "\n",
    "        self.s_nc  = s_nc\n",
    "        self.s_cor = s_cor\n",
    "        self.s_det = s_det\n",
    "\n",
    "\n",
    "        self.s_space = np.array([self.s_nc,self.s_cor,self.s_det])        \n",
    "        \n",
    "        #Build the X and D space:\n",
    "\n",
    "        self.X = [] #set of feasible states corresponding to the action\n",
    "        self.D = [] #set of feasible actions corresponding to the space\n",
    "        self.XD = [] #creat the vector of the state action pair\n",
    "        self.X_indices = [] #Vector of indices from unique states in X\n",
    "        #self.X_unique = []# Vector with unique elements of X\n",
    "        \n",
    "        self.count = 0\n",
    "        self.index = 0\n",
    "        for i in self.wi_space:\n",
    "            for j in self.wj_space:\n",
    "                for k in self.s_space:\n",
    "                    for l in self.di_space:\n",
    "                        if k == s_det and (l == 1 or self.dj(np.asarray([i,j,k]),l) == 1): #rule 1\n",
    "                            pass\n",
    "                        elif (i < self.phi('payer') and l == 1) or (j < self.phi('receiver') and self.dj(np.asarray([i,j,k]),l) == 1) : #rule2\n",
    "                            pass\n",
    "                        elif k == s_cor and (i < self.pi('payer') + self.y0 or j < self.pi('receiver') + self.y0): #rule3\n",
    "                            pass\n",
    "                        elif k == s_det and (i > self.w_bar - self.phi('payer') - self.F + self.y0  or j > self.w_bar - self.phi('receiver') - self.F + self.y0): #rule4\n",
    "                            pass\n",
    "                        else:\n",
    "                            self.X.append([i,j,k])\n",
    "                            self.D.append(l)\n",
    "                            self.XD.append([i,j,k,l])\n",
    "                            #if the states are ordered, then the clauses bellow gives the X_indices\n",
    "                            if self.index == 0:\n",
    "                                self.X_indices.append(self.count)\n",
    "                            elif self.X[self.index] == self.X[self.index-1]:\n",
    "                                self.X_indices.append(self.count)\n",
    "                            else:\n",
    "                                self.count += 1\n",
    "                                self.X_indices.append(self.count)\n",
    "                            self.index += 1\n",
    "\n",
    "\n",
    "        #                         X = np.vstack([X,(i,j,k)])\n",
    "        #                         D = np.vstack([D,(l,m)])\n",
    "        #                         XD= np.vstack([XD ,(i,j,k,l,m)])\n",
    "        self.X = np.asarray(self.X) # converts integers to string\n",
    "        self.D = np.asarray(self.D)\n",
    "        self.XD = np.asarray(self.XD)\n",
    "        \n",
    "\n",
    "        \n",
    "        #Calculating the indexes #Calculating the indices\n",
    "        #Indices are the maps unique actions and states\n",
    "\n",
    "        self.X_unique, self.indices = np.unique(self.X, axis=0, return_index=True) #new X space, only with unique feasible states\n",
    "\n",
    "        self.X_unique = self.X_unique[np.argsort(self.indices)] #return unique elements and maintain the original order\n",
    "\n",
    "        self.X_indices = np.asarray(self.X_indices)\n",
    "\n",
    "        self.D_unique, self.indices = np.unique(self.D, axis=0, return_index=True) #new X space, only with unique feasible states\n",
    "\n",
    "        self.D_unique = self.D_unique[np.argsort(self.indices)] #return unique elements and maintain the original order\n",
    "\n",
    "        self.D_indices = self.D\n",
    " \n",
    "        #Calculating ther reward vector:\n",
    "        if self.player == 'payer':\n",
    "            self.R = []\n",
    "            for i in range(len(self.XD)):\n",
    "                self.R.append(self.reward(self.player, self.X[i][0], self.D[i] ,self.dj(self.X[i],self.D[i])  ) )\n",
    "        else:\n",
    "            self.R = []\n",
    "            for i in range(len(self.XD)):\n",
    "                self.R.append(self.reward(self.player, self.X[i][1], self.D[i] ,self.dj(self.X[i],self.D[i])  ) )\n",
    "        \n",
    "        #Calculating Omega Matrix\n",
    "        self.Omega = np.zeros((len(self.X),len(self.X_unique))) #this omega is calculated using the dj as given\n",
    "\n",
    "        #finding the X_unique prime from the pair XD , dj:\n",
    "        for i in range(len(self.XD)):\n",
    "            if self.s_prime(self.XD[i][3],self.dj(self.X[i],self.XD[i][3])) == self.s_nc or self.XD[i][2] == self.s_det: # rules 1, 6 and 7\n",
    "                self.pos_2 = self.s_nc\n",
    "                self.pos_0 = self.w_prime('payer',self.XD[i][0],self.XD[i][3],self.dj(self.X[i],self.XD[i][3]),self.s_nc)\n",
    "                self.pos_1 = self.w_prime('receiver',self.XD[i][1],self.dj(self.X[i],self.XD[i][3]),self.XD[i][3],self.s_nc)\n",
    "                self.loc_X_unique = np.array([self.pos_0, self.pos_1, self.pos_2])\n",
    "\n",
    "                self.X_unique_prime = np.where(np.all(self.X_unique==self.loc_X_unique,axis=1))[0][0]\n",
    "\n",
    "                self.Omega[i ,self.X_unique_prime] = 1\n",
    "\n",
    "            else:\n",
    "                #rules 4 and 5\n",
    "                self.pos_2 = self.s_det\n",
    "                self.pos_0 = self.w_prime('payer',self.XD[i][0],self.XD[i][3],self.dj(self.X[i],self.XD[i][3]),self.s_det)\n",
    "                self.pos_1 = self.w_prime('receiver',self.XD[i][1],self.dj(self.X[i],self.XD[i][3]),self.XD[i][3],self.s_det)\n",
    "                self.loc_X_unique = np.array([self.pos_0, self.pos_1, self.pos_2])\n",
    "\n",
    "                self.X_unique_prime = np.where(np.all(self.X_unique==self.loc_X_unique,axis=1))[0][0]\n",
    "\n",
    "                self.Omega[i ,self.X_unique_prime] = self.alpha\n",
    "\n",
    "                #rules 2 and 3\n",
    "                self.pos_2 = self.s_cor\n",
    "                self.pos_0 = self.w_prime('payer',self.XD[i][0],self.XD[i][3],dj(self.X[i],self.XD[i][3]),self.s_cor)\n",
    "                self.pos_1 = self.w_prime('receiver',self.XD[i][1],self.dj(self.X[i],self.XD[0][3]),self.XD[i][3],self.s_cor)\n",
    "                self.loc_X_unique = np.array([self.pos_0, self.pos_1, self.pos_2])\n",
    "\n",
    "                self.X_unique_prime = np.where(np.all(self.X_unique==self.loc_X_unique,axis=1))[0][0]\n",
    "\n",
    "                self.Omega[i ,self.X_unique_prime] = 1 - self.alpha\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        self.end_time = datetime.now()\n",
    "        print('Duration: {}'.format(self.end_time - self.start_time))\n",
    "        ############################################################################\n",
    "        ######################## FUNCTIONS #########################################\n",
    "        ############################################################################\n",
    "        \n",
    "    \n",
    "        \n",
    "    def pi(self, player):\n",
    "        if player == 'payer':\n",
    "            return self.a\n",
    "        else:\n",
    "            return self.b\n",
    "\n",
    "\n",
    "    def phi(self, player):\n",
    "        if player == 'payer':\n",
    "            return self.b\n",
    "        else:\n",
    "            return self.c    \n",
    "\n",
    "\n",
    "    #laws of motion\n",
    "\n",
    "    #wealth state transition Function (as function of all other states)\n",
    "    def w_prime(self, player,w,ui,uj,s_prime): \n",
    "        if ui == 1 and uj == 1: # Note that s_nc after ui and uj == 1  does not exist\n",
    "            if s_prime == s_det:\n",
    "                if w >= self.F + self.phi(player) - self.y0:\n",
    "                    return w + self.y0 - self.F - self.phi(player) #return from detectiono\n",
    "                else:\n",
    "                    return 0\n",
    "            elif s_prime == s_cor:\n",
    "                if w <= self.w_bar - self.phi(player) - self.pi(player) - self.y0: \n",
    "                    return w + self.y0 + self.pi(player) - self.phi(player) # return from corruption\n",
    "                else: #if \n",
    "                    return self.w_bar\n",
    "        else:\n",
    "            if w == self.w_bar:\n",
    "                return w\n",
    "            elif w <= self.phi(player):\n",
    "                return w + self.y0 + 1 # living wage # just enough to gather the funds to enter in corruption\n",
    "            else:\n",
    "                return w + self.y0\n",
    "\n",
    "    #State of the world transition Function (as function of all other states)\n",
    "\n",
    "    def s_prime(self, ui,uj):\n",
    "        if ui == 0 or uj == 0:\n",
    "            return self.s_nc\n",
    "        else:\n",
    "            return (self.s_cor or self.s_det)\n",
    "\n",
    "    # The player i takes the action from j as given \n",
    "    # first iteration is a greedy strategy\n",
    "\n",
    "    def dj(self, X,di):\n",
    "        if X[2] == self.s_det:\n",
    "            return 0\n",
    "        elif (len(self.sigma)>0) and (X.tolist() in self.X_loc.tolist()):\n",
    "            return self.sigma[np.where((self.X_loc == (X)).all(axis=1))][0] # dj from sigma mapping. It extracts the index from a row in X and\n",
    "        else:\n",
    "            if self.player == 'payer':\n",
    "                if self.delta*((1-self.alpha)*self.pi(self.otherplayer) - self.alpha*self.F) > 0: # avoid workin with complex numbers\n",
    "                    if self.reward(self.otherplayer, X[1], di, 1) < self.delta* self.expected_return(self.otherplayer, X[1], di, 1):\n",
    "                        return 1\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                if self.delta*((1-self.alpha)*self.pi(self.otherplayer) - self.alpha*self.F) > 0: # avoid workin with complex numbers\n",
    "                    if self.reward(self.otherplayer, X[0], 1, di) < self.delta* self.expected_return(self.otherplayer, X[0], 1 , di):\n",
    "                        return 1\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return 0\n",
    "\n",
    "    #reward function:\n",
    "    def reward(self,player , wi, ui, uj):\n",
    "    #    return y - b*u #neutral risk aversion\n",
    "        if wi - self.phi(player)*ui*uj > 0:\n",
    "            return (((wi - self.phi(player)*ui*uj)**(1-self.eta))-1)/(1-self.eta) #CRRA function \n",
    "        else:\n",
    "            return 0\n",
    "    #expected return function:\n",
    "    def expected_return(self, player,wi,ui,uj):\n",
    "        if wi - self.phi(player)*ui*uj > 0:\n",
    "            return  ((((1-self.alpha)*self.pi(player) - self.alpha*self.F + wi )**(1-self.eta))-1)/(1-self.eta)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is possible to iterate the policies from a greedy policy up to the point where agents do not want to change their strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:06.027914\n",
      "Duration: 0:00:05.485355\n",
      "Success after 2 iterations\n"
     ]
    }
   ],
   "source": [
    "payer_sigma = np.zeros(len(results.sigma))\n",
    "next_player = 'receiver'\n",
    "count = 0\n",
    "while not np.array_equal(results.sigma, payer_sigma): #iterates until payer_sigma is equal to its last decision policy\n",
    "    player = next_player\n",
    "    instance = Matrices_Maker(results.sigma, X_loc = X_unique, player = player,c = 1, b=3 , a=5 , F=7 , w_bar = 30)\n",
    "    ddp =qe.markov.DiscreteDP(instance.R, instance.Omega, instance.delta, instance.X_indices, instance.D_indices)\n",
    "    results = ddp.solve(method='policy_iteration')\n",
    "    count += 1\n",
    "    X_unique = instance.X_unique\n",
    "    if player == 'payer':\n",
    "        next_player = 'receiver'\n",
    "        payer_sigma = results.sigma\n",
    "    else:\n",
    "        next_player = 'payer'  \n",
    "else:\n",
    "    print('Success after {} iterations'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:00.674196\n",
      "1.0 [10  7  0]\n"
     ]
    }
   ],
   "source": [
    "instance = Matrices_Maker( X_loc = X_unique, player = 'payer',c = 1, b=3 , a=5 , F=7 , w_bar = 30)\n",
    "ddp =qe.markov.DiscreteDP(instance.R, instance.Omega, instance.delta, instance.X_indices, instance.D_indices)\n",
    "results = ddp.solve(method='policy_iteration')\n",
    "#Stationary equilibrium\n",
    "for i in range(len(stationary[0])):\n",
    "    if stationary[0,i] != 0:\n",
    "        print(round(stationary[0,i],4), instance.X_unique[i])\n",
    "        \n",
    "#how to make it a zero summ game?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "* As it was supposed to happen, the players pay bribes untill that their utility from current wealth is lower than the expected walth gained from corrupion.\n",
    "\n",
    "* After this, they stop colluding at their wealth level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
